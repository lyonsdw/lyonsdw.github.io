<section xml:id="sampling_section"  xmlns:xi="http://www.w3.org/2001/XInclude">
<title>
Sampling
</title>

<!-- moved to sample variables section 11/11/2025
<subsection>
  <title>Simple random sample variables</title>
  
  <p>Let <m>(\Omega,P)</m> be a finite probability space
    with <m>N=|\Omega|</m> and with constant probability
    function <m>p(\omega)=\frac{1}{N}</m> for
    all <m>\omega\in\Omega</m>. Recall that <m>\Omega^{n\ast}</m>
    denotes the set of all one-to-one sequences of length <m>n</m>
    in <m>\Omega</m>,
    called <em>simple random samples</em> of size <m>n</m> taken
    from <m>\Omega</m>. Recall that the probability
    function <m>p_{\Omega^{n\ast}}</m> is constant, with constant
    value <m>\frac{1}{N(N-1)\cdots (N-n+1)}</m>. Given a random
    variable <m>X\colon \Omega \to \R</m>, we define sample random variables
    <m>X_1,X_2,\ldots,X_n</m> on <m>\Omega^{n\ast}</m> by the same
    formula <xref ref="sampvardef"/>as for ordinary sample variables. We
    call the <m>n</m>-tuple <m>(X_1,X_2,\ldots,X_n)</m> of variables
    on <m>\Omega^{n\ast}</m> a <term>simple random sample</term> of
    size <term></term> <m>n</m> of <m>X</m>.  As for ordinary samples,
    the simple random sample variables look like copies of <m>X</m>. 
    However, in contrast with the
    ordinary sample case, the simple random sample variables <m>X_k</m>
    are <em>dependent</em>. Here are some key properties.
 </p>

 <proposition>
   <title>Simple random samples of a random variable</title>
   <p>Let <m>(\Omega,P)</m> be a finite probability space with <m>N=|\Omega|</m>
     and with a constant probability function.
     Let <m>(X_1,X_2,\ldots,X_n)</m> be a simple random sample of a random
    variable <m>X\colon \Omega\to \R</m> with <m>E(X)=\mu</m>
    and <m>\var(X)=\sigma^2</m>. 
    We have the following.
    <mdn>
      <mrow>F_{X_k} \amp= F_X \text{   for }1\leq k\leq n</mrow>
      <mrow>E(X_k)\amp = \mu \text{   for }1\leq k\leq n</mrow>
      <mrow>\var(X_k)\amp = \sigma^2 \text{   for }1\leq k\leq n</mrow>                  
      <mrow xml:id="covarsimplesampvars">\covar(X_i,X_j)\amp =
	-\frac{\sigma^2}{N-1}\text{ if }i\neq j</mrow>
      <mrow xml:id="sesimpsampave">\var(\overline{X})\amp = \frac{\sigma^2}{n}\left(\frac{N-n}{N-1}\right)</mrow>      
      <mrow xml:id="varsumsimplesampvars">\var(n\overline{X})\amp =
	n\sigma^2\left(\frac{N-n}{N-1}\right)</mrow>

      -->
      
          <!-- see Rice, Mathematical statistics and data analysis, 3rd ed.
           p.212 Cor A -->
<!--
      <mrow>E\left(s^2\frac{N-n}{N}\right)\amp = \sigma^2</mrow>
    </mdn>
    </p>
 </proposition>

   <p><alert>Terminology:</alert> Simple random samples are also
    called <em>samples taken without replacement</em>, or <em>survey
      samples</em>. This refers to the applied scenario in which the
    probability space <m>\Omega</m> models a human population, where
    each individual in the population has the same chance of being
    selected for a survey (or some kind of measurement). Once surveyed,
    that individual will not be surveyed again; in other words, survey
     samples produce one-to-one sequences. As for ordinary samples, the
     quantities <m>\sigma_{\overline{X}},\sigma_{n\overline{X}}</m> (the
    square roots of the variances <xref ref="sesimpsampave"/> and <xref ref="varsumsimplesampvars"/>,
     respectively) are called <em>standard errors</em>.
     The
    quantity <m>\sqrt{\frac{N-n}{N-1}}</m> that occurs in both
    standard errors is called the <term>correction factor</term> for the
    standard errors when sampling without replacement (or for simple
    random sampling, or for survey sampling).
  </p>

<aside component="instructor"><title>Some derivations</title>   
  <p><alert>A derivation of <xref ref="covarsimplesampvars"/>.</alert>
    Start with
    <mdn><mrow>E^\ast(X_1X_2)\amp =\sum_{(\omega_1,\ldots,\omega_n)\in\Omega^{n\ast}}X(\omega_1)X(\omega_2)\frac{1}{N(N-1)\cdots
	(N-n+1)}</mrow>
      <mrow xml:id="estarx1x2">\amp= \frac{1}{N(N-1)}\sum_{\omega_1\neq\omega_2}X(\omega_1)X(\omega_2).</mrow>
    </mdn> 
Next consider
    <men>\left(\sum_{\omega} X(\omega)\right)^2=\sum_{\omega} (X(\omega))^2
      + \sum_{\omega\neq \omega'}X(\omega)X(\omega').</men>
    Multiply through by <m>\frac{1}{N(N-1)}</m> and substitute <xref ref="estarx1x2"/>.
    <men>\frac{N}{N-1}\left(\frac{\sum_{\omega} X(\omega)}{N}\right)^2=
      \frac{1}{N-1}E(X^2)+ E^\ast(X_1X_2).</men>
    Now we have
    <mdn>
      <mrow>\covar^\ast(X_1,X_2) \amp =
      \frac{1}{N-1}\left(N\mu^2-E(X^2)\right) - \frac{N-1}{N-1}\mu^2
      </mrow>
      <mrow>\amp = \frac{1}{N-1}\left(\mu^2 - E(X^2)\right) = -\frac{\sigma^2}{N-1}.
      </mrow>
    </mdn>
  </p>


  <p><alert>A derivation of <xref ref="varsumsimplesampvars"/>.</alert>
    Start with
    <mdn>
      <mrow>\var\left(\sum_k X_k\right) \amp = \sum_k \var(X_k) + 2\sum_{j\neq k} \covar(X_j,X_k)
      </mrow>
      <mrow>\amp = n\sigma^2  -2\frac{n(n-1)}{2}\left(\frac{\sigma^2}{N-1}\right)</mrow>
    </mdn>
    (by <xref ref="covarsimplesampvars"/>), then simplify.
  </p>
</aside>

  <exercise>
    <p>exercises in definitions, derivations, examples</p>
    </exercise>
    
</subsection>
-->

<subsection><title>The Central Limit Theorem</title>

  <p>Recall that <m>\Phi</m> denotes the cumulative distribution
  function for the standard normal distribution.
  </p>
  
  <proposition>
    <title>The Central Limit Theorem</title>
    <p>
      Let <m>X_1,X_2,X_3,\ldots</m> be a sequence of sample variables
      for a random variable <m>X</m> with <m>E(X)=\mu</m> and <m>\var(X)=\sigma^2</m>.
      Let <m>S_n</m> denote the
      variable <m>S_n=X_1+X_2+\cdots + X_n</m>, so that we
      have <m>E(S_n)=n\mu</m>
      and <m>\var{S_n}=n\sigma^2</m>. Let <m>T=(S_n-n\mu)/(\sqrt{n}\sigma)</m>. We
      have
      <me>\lim_{n\to\infty} F_T(x) = \Phi(x)</me>
      for all <m>x \in \R</m>.
    </p>
  </proposition>
    <exercise>
    <p>exercises in definitions, examples</p>
    </exercise>
    
</subsection>

<subsection><title>Estimating population parameters</title>

  <p>Let <m>X</m> be a random variable with <m>E(X)=\mu</m>
  and <m>\var(X)=\sigma^2</m>. Let <m>\overline{X}=\frac{1}{n}\sum_{i=1}^n X_i</m> be the average of
  a sample of size <m>n</m> of <m>X</m>. For <m>\alpha</m> in the
  range <m>0\lt \alpha \lt 1</m>, let <m>z_\alpha</m> be the value of a
  standard normal variable such that <m>P(|z|\leq z_\alpha)=\alpha </m>.
  The Central Limit Theorem says that
    <men xml:id="cltinspireconfint">P(|\overline{X}-\mu| \leq z_\alpha \sigma_{\overline{X}})
      \approx \alpha.</men>
    Equation <xref ref="cltinspireconfint"/> motivates the following
    procedure for estimating an unknown population parameter <m>\mu</m>
    from sample data. Intuitively, <m>\overline{X}</m>
    estimates <m>\mu</m> with an error estimated
    by <m>\sigma_{\overline{X}}</m>. If the population
    variance <m>\sigma^2</m> is unknown, we can use the sample
    SD <m>s</m> to estimate <m>\sigma</m>, and we can
    use <m>s/\sqrt{n}</m> to
      estimate <m>\sigma_{\overline{X}}</m>. Inspired by
    <xref ref="cltinspireconfint"/>, we say that the interval
    <men>\overline{X}\pm z_\alpha s/\sqrt{n}</men>
    is a <m>100z_\alpha\%</m> confidence interval for the population
    mean <m>\mu</m>. Notice that the confidence interval is a random
    quantity, that is, it depends on the sample. Roughly speaking, we
    estimate that approximately <m>100z_\alpha\%</m> of
    all <m>100z_\alpha\%</m> confident intervals will
    contain <m>\mu</m>, and approximately <m>100(1-z_\alpha)\%</m> of
    these intervals will fail to contain <m>\mu</m>.
  </p>
  
    <exercise>
      <p>exercises in definitions, examples

	<ol>
	<li>Show that <m>z_\alpha = \Phi^{-1}((\alpha+1)/2)</m>.
	</li>
	<li>Explain why <xref ref="cltinspireconfint"/> holds.</li>
	</ol>
	show that 

      </p>
    </exercise>
    
</subsection>



</section>
