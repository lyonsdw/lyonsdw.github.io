<section xml:id="probability_finite_section"  xmlns:xi="http://www.w3.org/2001/XInclude">
<title>
  Finite Probability Models
</title>

<introduction>  <p>Probability models are mathematical frameworks that help us think
  about chance processes whose outcomes cannot be predicted
  with certainty. We begin with one of the simplest
  types of probability models, in which the set of possible outcomes of
    a chance process is finite.
  </p>
</introduction>

<subsection><title>Vocabulary and Properties</title>
  
  <definition xml:id="finprobfndef">
    <p>A <term>finite probability function</term> is a function
      <m>p\colon \Omega	\to [0,1]</m>, where <m>\Omega</m> is a finite set,
      and where <m>p</m> satisfies
      <men xml:id="probfunsum1">\sum_{\omega\in\Omega} p(\omega)=1.</men>
    </p>
  </definition>

  <p>The next definition extends the assignments of probabilities to <em>subsets</em> of <m>\Omega</m>.
    Recall that the <em>power set</em> <m>\mathcal{P}(S)</m> of a
    set <m>S</m> is the set of all subsets of <m>S</m>.
  </p>

  <definition xml:id="finprobmeasdef">
    <p>A <term>finite probability measure</term> is a function
      <m>P\colon \mathcal{P}(\Omega)\to [0,1]</m> be given by
    <men xml:id="discreteprobmeasdef">P(E)=\sum_{\omega \in E} p(\omega)</men>
    for <m>E\subseteq \Omega</m>, where <m>p\colon \Omega\to [0,1]</m> is a finite probability function.
</p>
  </definition>

    <p><alert>Comment on notation:</alert> For a single
    element <m>\omega</m> in <m>\Omega</m>, it is common practice to
      write <m>P(\omega)</m> as a shorthand
      for <m>P(\{\omega\})</m>.
    </p>

  
  <definition xml:id="finiteprobmodeldef"><title>Finite probability model</title>
    <p>
      A <term>finite probability model</term> is a
	pair <m>(\Omega,P)</m>, where <m>\Omega</m>
	is a finite set, and where <m>P</m> is a finite
  probability measure on <m>\Omega</m>.
      The set <m>\Omega</m> is called
    the <term>probability space</term> of the model. Elements
    of <m>\Omega</m> are called <term>outcomes</term>, and elements
  of <m>\mathcal{P}(\Omega)</m> are called <term>events</term>.</p>
  </definition>
  
    <exercise xml:id="probfnexer">
      <p>Write the probability function for the probability space <m>\Omega=\{a,b,c\}</m>
	where outcome <m>b</m> is twice as likely as outcome <m>a</m>,
	and outcome <m>c</m> is three times as likely as outcome <m>b</m>.
      </p>
    </exercise>

<aside component="instructor">
  <title>Instructor solution for <xref ref="probfnexer"/></title>
  <p>
    Let <m>p(a)=1/9</m>, <m>p(b)=2/9</m>, and <m>p(c)=6/9</m>.
  </p>
</aside>
    

  <definition xml:id="finprobmodelvocabdef"><title>More probability terminology</title>
  <p>Let <m>(\Omega,P)</m> be a finite probability model.
    The complement <m>A^c=\Omega\setminus A</m> of an event <m>A</m>
    is also called the <term>opposite</term> of <m>A</m>. Disjoint
    events <m>A,B</m> (that is, <m>A\cap B=\emptyset)</m>
    are also called <term>mutually exclusive</term>.
    Given events <m>U,V</m> with <m>P(V)\neq 0</m>, the <term>conditional probability
      of <m>U</m> given <m>V</m></term>, denoted <m>P(U|V)</m>, is
    defined to be
    <men xml:id="condprobdef">P(U|V)=\frac{P(U\cap V)}{P(V)}.</men> Events <m>U,V</m> are
    called <term>independent</term> if <m>P(U\cap
      V)=P(U)P(V)</m>. Otherwise, <m>U,V</m> are
    called <term>dependent</term>.
    </p>
  </definition>

      <exercise xml:id="finiteindepdepexer">
	<p>Let <m>\Omega=\{a,b,c\}</m>, let <m>U=\{a,b\}</m>, and
	let <m>V=\{b,c\}</m>. Give an example of a probability function
	on <m>\Omega</m> for which <m>U,V</m> are independent. Give an
	example of a probability function on <m>\Omega</m> for
	which <m>U,V</m> are dependent.
	</p>
      </exercise>

      <aside component="instructor">
  <title>Instructor solution for <xref ref="finiteindepdepexer"/></title>
  <p>
    For independence, let <m>p(a)=p(c)=0</m> and let <m>p(b)=1.</m> Then
    we have <m>P(U)P(V)=1=P(U\cap V)</m>. For dependence,
    let <m>p(a)=p(c)=1/2</m> and let <m>p(b)=0</m>. Then we
    have <m>P(U)P(V)=1/4\neq P(U\cap V)</m>.
  </p>
</aside>

      
      <proposition xml:id="probprops">
	<title>Properties of probability</title>
    <p>Let <m>(\Omega,P)</m> be a finite probability model. The following
      properties hold.
      <ol>
      <li><m>P(\Omega)=1</m>.
      </li>
      <li>If <m>A\subseteq B</m> for some events <m>A,B</m>,
      then <m>P(A)\leq P(B)</m>.
      </li>      
      <li><term>(addition rule)</term> If events <m>A,B</m> are disjoint, then <m>P(A\cup B)=P(A)+P(B)</m>.
      </li>
      <!--
      <li>
	For any two events <m>A,B</m>, we have <m>P(A\cup
      B)=P(A)+P(B)-P(A\cap B)</m>.
      </li>
      -->
      <li><term>(opposite rule)</term>
	For any event <m>A</m>, we have <m>P(A^c)=1-P(A)</m>.
      </li>
      <li><term>(multiplication rule)</term>
	For any events <m>U,V</m> such that <m>P(U)\neq 0</m>, we
      have <m>P(U\cap V) = P(U)P(V|U)</m>.
      </li>
      <li>Let <m>U,V</m> be events with <m>P(U)\neq 0</m>.
	Events <m>U,V</m> are independent if and only if
	<m>P(V)=P(V|U)</m>.
      </li>
      </ol>
    </p>
  </proposition>

    <exercise>
    <p>Prove all the parts of <xref ref="probprops"/>.</p>
  </exercise>

    <aside component="instructor">
  <title>Instructor solution for <xref ref="probprops"/></title>
  <p>
      <ol>
      <li> We have <me>P(\Omega)=\sum_{\omega\in\Omega}p(\omega) =
	  1.</me> The first equality is by
	  <xref ref="discreteprobmeasdef"/> from the definition of probability
	  measure. The second equality is by <xref ref="probfunsum1"/>
	  in the definition of the probability function <m>p</m>.
      </li>
      <li>We have <me>P(A)=\sum_{\omega\in A}p(\omega) \leq
	\sum_{\omega\in A} p(\omega)+ \sum_{\omega\in B\setminus
	  A}p(\omega) = P(B)</me> because all values of <m>p</m> are
	nonnegative. Alternatively, prove part 3 first, then
	write <m>B</m> as the disjoint union <m>B=B\cup (B\setminus A)</m>.
      </li>      
      <li>We have <me>P(A\cup B)=\sum_{\omega\in A\cup B}p(\omega) =
	\sum_{\omega \in A)p(\omega) + \sum_{\omega\in B}p(\omega) =
	P(A)+P(B).</me> This is just splitting the summands into two
	disjoint collections.
      </li>
      <li>Applying parts 1 and 3 to <m>\Omega= A\cup A^c</m>
      yields <m>1=P(A)+P(A^c)</m>. Rearranging gives the desired result.
      </li>
      <li>This is just a rearrangement of the definition of conditional probability.
      </li>
      <li>If <m>U,V</m> are independent, then <m>P(U)P(V|U)=P(U\cap V)=
	P(U)P(V)</m>. The first equality is part 5, and the second
	equality is the definition of
	independence. Cancelling <m>P(U)\neq 0</m>, we have <m>P(V)=P(V|U)</m>.
      </li>
      </ol>
  </p>
</aside>

    
</subsection>


<subsection xml:id="finiterandsampsect">
  <title>Random Samples</title>

    <p>Let <m>(\Omega,P)</m> be a finite probability model with
    probability function <m>p</m>, and let <m>n</m> be a positive integer. Let <m>p_{\Omega^n}\colon
      \Omega^n\to [0,1]</m> be defined by
    <men xml:id="randsampprobfn">p_{\Omega^n}(\omega_1,\omega_2,\ldots,\omega_n)=p(\omega_1)p(\omega_2)\cdots
      p(\omega_n).</men> It is easy to check (see <xref ref="productprobfnexer"/> below)
    that <m>p_{\Omega^n}</m> is a probability
      function. Let <m>P_{\Omega^n}</m> denote the corresponding
    probability measure. The probability
    model <m>(\Omega^n,P_{\Omega^n})</m> is called the space
    of <term>(random) samples of size <m>n</m>
    </term> taken from the space <m>\Omega</m>. 
    The space <m>\Omega^n</m> models the outcomes that are obtained by
    <m>n</m> repetitions of the chance process that produces
    outcomes in <m>\Omega</m>.
</p>

  <exercise xml:id="productprobfnexer">
    <p>
      <ol><li>Show that <m>p_{\Omega^n}</m> is a probability
	  function.</li>
	<li>Let <m>E</m> be an event in <m>\Omega</m>, and let <m>E'</m>
	  be the event <m>E'=\{\vec{\omega}\colon\omega_j\in E\}</m>
	  in <m>\Omega^n</m>, where <m>\vec{\omega}</m>
	  denotes <m>\vec{\omega}=(\omega_1,\omega_2,\ldots,\omega_n)</m>. Show
	  that
	  <me>p_{\Omega^n}(E') = p_{\Omega}(E).</me>
	</li>
<li>Let <m>E,F</m> be events in <m>\Omega</m>, and
      let <m>E',F'</m> be the events <m>E'=\{\vec{\omega}\colon\omega_j\in E\}</m>
      and <m>F'=\{\vec{\omega}\colon\omega_k\in F\}</m>. Show that <m>E',F'</m> are
	  independent in <m>\Omega^n</m> if <m>j\neq k</m>.</li>
      </ol>
    </p>
  </exercise>

  <aside component="instructor">
  <title>Instructor solution for <xref ref="productprobfnexer"/></title>
  <p>
    <ol>
      <li>It is clear that the values of <m>p_{\Omega^n}</m> are in the
	set <m>[0,1]</m>. We check that the values sum to 1. Indeed, we
	have
	<me>\sum_{\vec{\omega}\in \Omega^n}p_{\Omega^n}(\vec{\omega})=
	\sum_{\vec{\omega}}\prod_{k=1}^n p(\omega_k) = \prod_{k=1}^n
	  \sum_{\omega\in\Omega}p(\omega) = 1</me>
	where <m>\vec{\omega}</m>
	denotes <m>(\omega_1,\omega_2,\ldots,\omega_n)</m>. The
	inequality in the middle is just a rearrangement of the terms in
	the sum.
      </li>
      <li>We have
	<me>P_{\Omega^n}(E')
	  = \sum_{\vec{\omega}\colon \omega_j\in E}p(\vec{\omega})
	  = \left(\sum_{\omega_j\in E}p(\omega_j)\right)
	  \left(\sum_{\vec{\omega}_{j}}p_{\Omega^{n-1}}(\vec{\omega}_{j})\right)
	  = P_{\Omega}(E)\cdot 1
	</me>
	where <m>\vec{\omega}_j</m> denotes
	the <m>(n-1)</m>-tuple <m>(\omega_1,\omega_2,\ldots,\widehat{\omega_j},\ldots,\omega_n)</m>
	(that is, <m>\vec{\omega}</m> with the <m>j</m>-th entry
	deleted). As in the previous part, this is just a rearrangement
	of terms in the sum.
      </li>
      <li>Again, the derivation is a rearrangement of terms. This one
      involves the factoring (for <m>j\neq k</m>)
	<me>
	  \sum_{\vec{\omega}\colon \omega_j\in E, \omega_k\in F}p(\vec{\omega}) =
	  \left(\sum_{\omega_j\in E}p(\omega_j)\right)
	  \left(\sum_{\omega_k\in F}p(\omega_k)\right)
\left(\sum_{\vec{\omega}_{j,k}}p_{\Omega^{n-2}}(\vec{\omega}_{j,k}\right)	  
	</me>
	where <m>\vec{\omega}_{j,k}</m> denotes <m>\vec{\omega}</m> with
	the <m>j</m>-th and the <m>k</m>-th entries deleted.
	</li>
      </ol>
  </p>
</aside>

  
</subsection>

<subsection>
  <title>Simple Random Samples</title>

  <p>Let <m>(\Omega,P)</m> be a finite probability space
    with <m>N=|\Omega|</m>, and with the constant
    probability function <m>p(\omega)=\frac{1}{N}</m> for
    all <m>\omega\in \Omega</m>. Let <m>\Omega^{n\ast}</m> denote the
    set of all one-to-one sequences<fn>We use the nonstandard
      notation <m>\Omega^{n\ast}</m>, rather than the standard
      notation <m>P_n(\Omega)</m>, to denote the set of permutations
      of <m>n</m> elements of <m>\Omega</m>, in order to avoid confusion
    with probability measures, which are also denoted using capital
    letter P.
    </fn>
    in <m>\Omega</m> of size <m>n</m>.
    An element <m>(\omega_1,\omega_2,\ldots,\omega_n)</m>
    of <m>\Omega^{n\ast}</m> is called a
    <term>simple random sample of size <m>n</m>
    </term> taken from a probability
    space <m>\Omega</m>. Let <m>p_{\Omega^{n\ast}}</m> be given by the
    constant function
    <men>p_{\Omega^{n\ast}}(\omega_1,\omega_2,\ldots,\omega_n)=\frac{1}{N(N-1)\cdots
      (N-n+1)}</men>
    for all <m>(\omega_1,\omega_2,\ldots,\omega_n)\in
      \Omega^{n\ast}</m>. It is easy to check (see <xref ref="simprandsampprobfnexer"/> below)
    that <m>p_{\Omega^n}</m> is a probability
      function. Let <m>P_{\Omega^{n\ast}}</m> denote the corresponding
    probability measure. The probability
    model <m>(\Omega^n,P_{\Omega^{n\ast}})</m> is called the space
    of <term>simple random samples of size <m>n</m> 
    </term> taken from the space <m>\Omega</m> (or the space of samples of size <m>n</m>
    taken from <m>\Omega</m>
    <term>without replacement</term>).

  </p>

    <exercise xml:id="simprandsampprobfnexer">
      <p>
	<!--Show that <m>p_{\Omega^{n\ast}}</m> is a probability
	  function.-->
     <ol><li>Show that <m>p_{\Omega^{n\ast}}</m> is a probability
	  function.</li>
	<li>Let <m>E</m> be an event in <m>\Omega</m>, and let <m>E'</m>
	  be the event <m>E'=\{\vec{\omega}\colon\omega_j\in E\}</m>
	  in <m>\Omega^n</m>, where <m>\vec{\omega}</m>
	  denotes <m>\vec{\omega}=(\omega_1,\omega_2,\ldots,\omega_n)</m>. Show
	  that
	  <me>P_{\Omega^{n\ast}}(E') = P_{\Omega}(E).</me>
	</li>
<li>Let <m>E,F</m> be events in <m>\Omega</m>, and
      let <m>E',F'</m> be the events <m>E'=\{\vec{\omega}\colon\omega_j\in E\}</m>
      and <m>F'=\{\vec{\omega}\colon\omega_k\in F\}</m>. Show by example
  that that <m>E',F'</m> may be <em>dependent</em> in <m>\Omega^{n\ast}</m> if <m>j\neq k</m>.</li>
      </ol>
      </p>
  </exercise>

       <aside component="instructor">
  <title>Instructor solution for <xref ref="simprandsampprobfnexer"/></title>
  <p>
<ol>    <li>It is clear that <m>p_{\Omega^{n\ast}}</m> takes values
    in <m>[0,1]</m>. By <xref ref="permnrformula"/>, the size
    of <m>\Omega^{n\ast}</m> is <m>N(N-1)(N-2)\cdots (N-n+1)</m>, so we
    see that <m>\sum_{\vec{\omega}\in \Omega^{n\ast}}
      p_{\Omega^{n\ast}}(\vec{\omega})=1</m>.</li>
  <li>The main observation here (make it a Lemma, maybe) is that <m>|E'|=|E|(N-1)(N-2)\cdots
      (N-n+1)</m> (use a counting argument). Now we have
    <me>P_{\Omega^{n\ast}}(E')= \sum_{\vec{\omega}\in
    \Omega^{n\ast}}\frac{1}{N(N-1)\cdots (N-n+1)}=
      \frac{|E'|}{N(N-1)\cdots (N-n+1)}= \frac{|E|}{N}=P_\Omega(E)</me>
    where the next-to-last equality comes from applying the opening observation/lemma.
  </li>
  <li>Let <m>\Omega=\{H,T\}</m> with <m>p(H)=p(T)=1/2</m>,
    let <m>n=2</m>, let <m>j=1</m>, and
    let <m>k=2</m>. Let <m>E=\{H\}</m> and let <m>F=\{T\}</m>.
    We have
    <m>P(E'\cap F')=1/2</m> but <m>P(E')P(F')=1/4</m>.
  </li>
</ol>
  </p>
</aside>

</subsection>

<subsection><title>Bayes' Rule</title>

  <p><term>Bayes' Rule</term>
    can be viewed as a practical method
    for finding conditional probabilities. For events <m>A,B</m> in a
    probability space <m>\Omega</m>, where
    both probabilities <m>P(A),P(B)</m> are nonzero, the definition of
    conditional probability <xref ref="condprobdef"/> gives us two ways
    to write <m>P(A\cap B)</m>, namely
    <me>P(A\cap B)= P(A)P(B|A) = P(B)P(A|B).</me>
    Dividing the last two terms by <m>P(A)</m> leads to the following,
    which is a basic form of Bayes' Rule.
      <men xml:id="bayesrulebasic">  P(B|A) =\frac{P(B)P(A|B)}{P(A)}</men>      
    A use case for this version of Bayes' Rule is a problem in which you
    know the probabilities in the expression on the right, and you wish
    to find the probability on the left. In effect, this allows you to
    use <m>P(A|B)</m> to find <m>P(B|A)</m>.
  </p>

<p>In a more nuanced form of Bayes' Rule, the event <m>B</m> is one of
    of a collection of events <m>B_1,B_2,\ldots,
    B_r</m> that form a partition of <m>\Omega</m>, say, <m>B=B_k</m>. In this case, we
  have
  <mdn>
    <mrow xml:id="startbayes">A \amp = \bigcup_{i=1}^r (A\cap B_i)</mrow>
    <mrow xml:id="lawoftotalprobeqn">P(A) \amp = \sum_{i=1}^r P(A\cap B_i)</mrow>
    <mrow xml:id="partcondprob">\amp = \sum_{i=1}^r P(B_i)P(A|B_i)</mrow>        
  </mdn>
  Substituting <m>B=B_k</m>, and substituting the last expression <xref ref="partcondprob"/>
  for <m>P(A)</m> in <xref ref="bayesrulebasic"/>, we have the following form
  of Bayes' Rule.
  <men xml:id="bayesrule">P(B_k|A)  =  \frac{P(B_k)P(A|B_k)}{\sum_i P(B_i)P(A|B_i)}
  </men>
</p>

<p><alert>Terminology comment:</alert> Equations
  <xref ref="lawoftotalprobeqn"/> and <xref ref="partcondprob"/> are
  sometimes called the <em>law of total probability</em>.
</p>

<exercise xml:id="bayesintroexer">
  <p>
    <ol>
      <li>Justify all the steps in the derivation for Bayes'
	Rule.</li>
      <li>Find the sets <m>A</m> and <m>B_j</m>
	for medical testing scenario (what is the
	chance of having the disease given a positive test result?)
	</li>
    </ol>
  </p>
</exercise>

<aside component="instructor">
  <title>Instructor solution for <xref ref="bayesintroexer"/></title>
  <p>
<ol><li>Starting with <xref ref="startbayes"/>, we get
<xref ref="lawoftotalprobeqn"/> by applying the addition rule for the
disjoint events <m>A\cap B_i</m>. The next equality is the definition of
conditional probability. From there to the next and final expression
<xref ref="bayesrule"/> is explicitly described in the narrative.</li>
  <li>Let <m>A</m> be the event <q>tests positive</q>, let <m>B_1</m> be
    the event <q>has the disease</q> and let <m>B_2</m> be the
    event <q>does not have the disease</q>. We know the conditional
    probabilities <m>P(B_i|A)</m>. Bayes' rule helps us find the unknown
    conditional probability <m>P(B_1|A)</m> in terms of the known
    conditional probabilities.
  </li>
</ol>
  </p>
</aside>

</subsection>


<exercises><title>Probability Exercises</title>

<exercise> <p> Do exercises 1 and 2
      in <url href="https://mathvista.org/elemstats_notes/basic_probability.html">Section
      1.7.2</url> of <xref ref="lyons_elemstats"/>.
  </p>
</exercise>

<exercise><p>
    <url href="https://quantum.lvc.edu/lyons/courses/mas270_supp_materials/probability_problem_set.pdf">More exercises with solutions.
    </url>
  </p>
</exercise>

<exercise xml:id="probexerrossandwright"><p>Work the <q>Probability Exercises</q>
    set posted on Canvas.</p>
</exercise>

  <aside component="instructor">
  <title>Instructor comment for Probability Exercises <xref ref="probexerrossandwright"/></title>
  <p>
Note Spring 2026: using exercises from Section 5.2 of
<xref ref="ross_and_wright"/>. Exercises and solutions to odd-numbered
problems are posted on Canvas.
  </p>
</aside>

</exercises>

</section>
